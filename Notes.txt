Hadoop Lecture 
- Most used in big data 
- It is actually a large scale batch data processing system 
* Why Hadoop 
- 
* Hadoop Features
- Store files through HDFS 
- Hadoop provides access to file system
- Contain mostly Java files and scripts 
- Provide source code and documentation 

* Hadoop Architecture 
- Process and analyse large files- (Pig)

*Hadoop High Level Architechture 

HDFS - Data node 
- Name node connects with the data node. The name node job is to privent data lost.
- 
*Rejects 
Home work to look up rejects and how does it works.

-------------------------------------------------
HDP
- Hadoop is built around orizontal scalling. 
- Horizontal scaling means that you scale by adding more machines into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine.

- Rejects 
pattern = r"^gr.*y$" #beging and end. 
----------------------------------------------

- Hadoop
Namenode Resilience 
what are Hadoop clusters?
Hadoop clusters are known for boosting the speed of data analysis applications. They also are highly scalable: If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput. Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes, which ensures that the data is not lost if one node fails.

How does HDFS configurate a cluster?
An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on.

How the files on a Hadoop eco system works?

Will the client be on one cluster? NO
---------------------------
python -m  python pip install jupyter 

installing Jupyter

https://www.digitalocean.com/community/tutorials/how-to-set-up-jupyter-notebook-with-python-3-on-ubuntu-18-04

Slides

https://slides.com/praketasaxena/deck-1d59f0/fullscreen#/3

How to install Microsoft teams
https://linuxize.com/post/how-to-install-pip-on-ubuntu-18.04/
-------------------------------------------------------------------------------------
HDFS - Write about HDFS
HDFS is a distributed file system that handles large data sets running on commodity hardware. It is used to scale a single Apache Hadoop cluster to hundreds (and even thousands) of nodes. HDFS is one of the major components of Apache Hadoop, the others being MapReduce and YARN.

Apache Hadoop
Apache Hadoop is a collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model.
----------------------------------------------------------

what is the difference between multiprocessesing and multithreading.
A multiprocessing system has more than two processors whereas Multithreading is a program execution technique that allows a single process to have multiple code segments. Multiprocessing improves the reliability of the system while in the multithreading process, each thread runs parallel to each other.
- Multithreading system is not interruptible/killable
- Multiprocessig (IPC(Inter-Process Communication) a quite complicated with more overhead. Has a larger memory footprint.
- Multiprocessing helps you to increase computing power.
- Multithreading helps you to create computing threads of a single process to increase computing power.

#Recap
1.In Multiprocessing, CPUs are added for increasing computing power.	While In Multithreading, many threads are created of a single process for increasing computing power.
2.	In Multiprocessing, Many processes are executed simultaneously.	While in multithreading, many threads of a process are executed simultaneously.
3.	Multiprocessing are classified into Symmetric and Asymmetric.	While Multithreading is not classified in any categories.
4.	In Multiprocessing, Process creation is a time-consuming process.	While in Multithreading, process creation is according to economical.
- 
The threading module uses threads, the multiprocessing module uses processes. The difference is that threads run in the same memory space, while processes have separate memory. This makes it a bit harder to share objects between processes with multiprocessing
==========================
Is python multiprocessing or multithreadng, through multiprocessing and multithreading.
The Python threading module uses threads instead of processes. Threads uniquely run in the same unique memory heap. Whereas Processes run in separate memory heaps. This makes sharing information harder with processes and object instances

--------------------------------------------------
manage vs external tables 
 The main difference is that when you drop an external table, the underlying data files stay intact. This is because the user is expected to manage the data files and directories. With a managed table, the underlying directories and data get wiped out when the table is dropped.

what is the difference between low cardinality and high cardinality?
In the context of databases, cardinality refers to the uniqueness of data values contained in a column. High cardinality means that the column contains a large percentage of totally unique values. Low cardinality means that the column contains a lot of “repeats” in its data range.

------------------------------
What is spark in hadoop?
What’s Spark?
Spark is a newer project, initially developed in 2012, at the AMPLab at UC Berkeley. It’s also a top-level Apache project focused on processing data in parallel across a cluster, but the biggest difference is that it works in-memory.

Whereas Hadoop reads and writes files to HDFS, Spark processes data in RAM using a concept known as an RDD, Resilient Distributed Dataset. Spark can run either in stand-alone mode, with a Hadoop cluster serving as the data source, or in conjunction with Mesos. In the latter scenario, the Mesos master replaces the Spark master or YARN for scheduling purposes.
-----------------------------
what is DAG in Spark

(Directed Acyclic Graph) DAG in Apache Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDD. In Spark DAG, every edge directs from earlier to later in the sequence.
----------------------------
wide transformation and narrow transformation

Narrow transformations are the result of map(), filter(). Wide transformation — In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD.

Apache Spark Shuffling
The process of moving the data from partition to partition in order to aggregate, join, match up, or spread out in some other way, is known as shuffling. The aggregation/reduction that takes place before data is moved across partitions is known as a map-side shuffle.

What is a Data Flow, Data Pipeline and ETL?

A data-flow diagram is a way of representing a flow of data through a process or a system. The DFD also provides information about the outputs and inputs of each entity and the process itself. A data-flow diagram has no control flow, there are no decision rules and no loops.

In computing, a pipeline, also known as a data pipeline, is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion.

In computing, extract, transform, load is the general procedure of copying data from one or more sources into a destination system which represents the data differently from the source or in a different context than the source.

What is Kafka?
Apache Kafka is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds.




